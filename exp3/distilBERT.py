# è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å®éªŒè„šæœ¬ï¼Œç”¨äº "å®éªŒä¸‰ï¼šå‰æ²¿äººå·¥æ™ºèƒ½ç®—æ³•ç ”ç©¶ä¸å®ç° - æ–¹å‘ä¸‰ (NLP)"
# ç›®æ ‡: æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è½»é‡çº§æ¨¡å‹ (DistilBERT) + é«˜æ•ˆå¾®è°ƒ (LoRA) + å¯è§£é‡Šæ€§ (SHAP)

# --- 1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“ ---
print("--- 1. å¼€å§‹å¯¼å…¥ä¾èµ–åº“ ---")
import torch
import torch.nn.functional as F
import numpy as np
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
import glob  # ç”¨äºæŸ¥æ‰¾æ–‡ä»¶
import sys  # ç”¨äºè·¯å¾„æ“ä½œ

import os  # ç”¨äºè·¯å¾„æ“ä½œ
import random  # ç”¨äºæ•°æ®å¢å¼º

# å¯¼å…¥ datasets åº“ï¼ˆé¿å…ä¸æœ¬åœ° datasets ç›®å½•å†²çªï¼‰
# ç”±äºexp3/datasets ç›®å½•ä¼šå¹²æ‰°å¯¼å…¥ï¼Œéœ€è¦ç¡®ä¿å¯¼å…¥çš„æ˜¯å®‰è£…çš„åº“
# æ‰€ä»¥ä¸´æ—¶ç§»é™¤å½“å‰ç›®å½•ä» sys.pathï¼Œå¯¼å…¥åå†æ¢å¤
_current_dir = os.path.dirname(os.path.abspath(__file__))
_removed_from_path = False
if _current_dir in sys.path:
    sys.path.remove(_current_dir)
    _removed_from_path = True

try:
    from datasets import load_dataset, Dataset  # ç”¨äºåŠ è½½ IMDb æ•°æ®é›†å’Œåˆ›å»ºæ•°æ®é›†
finally:
    # æ¢å¤ sys.pathï¼ˆå¦‚æœéœ€è¦ï¼‰
    if _removed_from_path:
        sys.path.insert(0, _current_dir)
from transformers import (
    AutoTokenizer,  # è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨
    AutoModelForSequenceClassification,  # è‡ªåŠ¨åŠ è½½åºåˆ—åˆ†ç±»æ¨¡å‹
    AutoModelForMaskedLM,  # ç”¨äº MLM é¢„è®­ç»ƒï¼ˆDAPTï¼‰
    TrainingArguments,  # è®­ç»ƒå‚æ•°é…ç½®
    Trainer,  # è®­ç»ƒå™¨
    DataCollatorWithPadding,  # åŠ¨æ€ padding æ•°æ®
    DataCollatorForLanguageModeling,  # ç”¨äº MLM é¢„è®­ç»ƒçš„æ•°æ®æ•´ç†å™¨
)
from peft import (
    get_peft_model,  # PEFT æ ¸å¿ƒå‡½æ•°ï¼Œç”¨äºåŒ…è£…æ¨¡å‹
    LoraConfig,  # LoRA é…ç½®
    TaskType,  # æŒ‡å®šä»»åŠ¡ç±»å‹
)
import evaluate  # hugging face çš„è¯„ä¼°åº“
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report  # è¯„ä¼°æŒ‡æ ‡
# æ³¨æ„ï¼šSHAP åˆ†æå·²ç§»è‡³ä¸“é—¨çš„ Notebook (SHAP_Analysis.ipynb)ï¼Œæ­¤å¤„ä¸å†å¯¼å…¥
import matplotlib.pyplot as plt  # ç”¨äºå¯è§†åŒ–
import matplotlib
import seaborn as sns  # ç”¨äºæ›´ç¾è§‚çš„å¯è§†åŒ–
from tqdm import tqdm  # ç”¨äºè¿›åº¦æ¡
import warnings
warnings.filterwarnings('ignore')

# é…ç½® matplotlib æ”¯æŒä¸­æ–‡æ˜¾ç¤º
font_list = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'FangSong', 'STSong', 'Arial Unicode MS']
available_fonts = [f.name for f in matplotlib.font_manager.fontManager.ttflist]
chinese_font = None
for font in font_list:
    if font in available_fonts:
        chinese_font = font
        break

if chinese_font:
    plt.rcParams['font.sans-serif'] = [chinese_font]
    # ç¡®ä¿matplotlibä½¿ç”¨ä¸­æ–‡å­—ä½“
    matplotlib.rcParams['font.sans-serif'] = [chinese_font]
else:
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
matplotlib.rcParams['axes.unicode_minus'] = False

# è®¾ç½®seabornæ ·å¼
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300

print("æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\n")

# --- 2. å®šä¹‰å…¨å±€é…ç½®å’Œè¾…åŠ©å‡½æ•° ---
# æŠŠä¸€äº›å›ºå®šå‚æ•°æ”¾åœ¨è¿™é‡Œï¼Œæ–¹ä¾¿ä¿®æ”¹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯

# è®°å½•ä¸€ä¸‹æˆ‘çš„æ€è€ƒï¼š
# é€‰æ‹© 'distilbert-base-uncased' ä½œä¸ºè½»é‡çº§æ¨¡å‹ï¼Œå› ä¸ºå®ƒæ¯” BERT-base å°å¾ˆå¤šï¼Œè®­ç»ƒå¿«ã€‚
# 'uncased' æ„å‘³ç€å®ƒä¸åŒºåˆ†å¤§å°å†™ã€‚

# æ¨¡å‹è·¯å¾„é…ç½®ï¼šæ”¯æŒç¦»çº¿æ¨¡å¼
# ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åœ¨çº¿æ¨¡å‹
import os
MODEL_NAME = "distilbert-base-uncased"
LOCAL_MODEL_PATH = "./models/distilbert-base-uncased"

# æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
if os.path.exists(LOCAL_MODEL_PATH) and os.path.exists(os.path.join(LOCAL_MODEL_PATH, "config.json")):
    MODEL_CHECKPOINT = LOCAL_MODEL_PATH
    print(f"æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼: {MODEL_CHECKPOINT}")
else:
    MODEL_CHECKPOINT = MODEL_NAME
    print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†å°è¯•åœ¨çº¿ä¸‹è½½: {MODEL_CHECKPOINT}")
    print(f"   æç¤ºï¼šå¦‚æœç½‘ç»œæœ‰é—®é¢˜ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹åˆ° {LOCAL_MODEL_PATH}")
    print(f"   è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹ OFFLINE_SETUP.md")
# æ•°æ®é›†å°±ç”¨å®éªŒæŒ‡å¯¼ä¹¦é‡Œæåˆ°çš„ IMDb
# æ³¨æ„ï¼šåªæ”¯æŒä»æœ¬åœ° parquet æ–‡ä»¶åŠ è½½ï¼Œä¸æ”¯æŒåœ¨çº¿ä¸‹è½½
LOCAL_DATASET_CACHE = "./datasets"  # æœ¬åœ°æ•°æ®é›†ç¼“å­˜ç›®å½•

# è®­ç»ƒé…ç½®ä¼˜åŒ–
NUM_EPOCHS = 5  # å¢åŠ åˆ°5ä¸ªepochï¼Œè®©æ¨¡å‹å……åˆ†è®­ç»ƒ
BATCH_SIZE = 16  # 16 æ¯”è¾ƒä¸­ç­‰ï¼Œä¸ä¼šå¤ªå æ˜¾å­˜

# ä¸åŒå®éªŒä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡
LEARNING_RATE_BASELINE = 2e-5  # Baseline å…¨é‡å¾®è°ƒä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
LEARNING_RATE_LORA = 3e-5  # LoRA ä½¿ç”¨é€‚ä¸­çš„å­¦ä¹ ç‡ï¼ˆé™ä½ä»¥æå‡ç¨³å®šæ€§ï¼Œé…åˆæ›´å¤§çš„rå€¼ï¼‰
LEARNING_RATE_DAPT_MLM = 5e-5  # DAPT é˜¶æ®µ1ï¼ˆMLMé¢„è®­ç»ƒï¼‰ä½¿ç”¨ç¨å¤§çš„å­¦ä¹ ç‡
LEARNING_RATE_DAPT_FINETUNE = 3e-5  # DAPT é˜¶æ®µ2ï¼ˆLoRAå¾®è°ƒï¼‰ä½¿ç”¨æ ‡å‡†å­¦ä¹ ç‡

# è®­ç»ƒæ•°æ®é‡é…ç½®ï¼ˆå¯è°ƒæ•´ä»¥æå‡æ•ˆæœï¼‰
TRAIN_SAMPLES = 10000  # å¢åŠ åˆ°10000æ¡è®­ç»ƒæ ·æœ¬ä»¥æå‡æ•ˆæœï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼š5000, 10000, 20000ç­‰ï¼‰
EVAL_SAMPLES = 2000  # å¢åŠ è¯„ä¼°æ ·æœ¬é‡ä»¥è·å¾—æ›´å¯é çš„è¯„ä¼°ç»“æœ


# å®éªŒè¦æ±‚ï¼šè¯„ä¼°æ€§èƒ½ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1 åˆ†æ•°ï¼‰
# æˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªè¯„ä¼°å‡½æ•°ï¼Œä¼ ç»™ Trainer
def compute_metrics(eval_pred):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
    """
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    # è®¡ç®— F1 score (macro)
    f1 = f1_score(labels, predictions, average="macro")
    # è®¡ç®— Accuracy
    acc = accuracy_score(labels, predictions)

    # å¿…é¡»è¿”å›ä¸€ä¸ªå­—å…¸
    return {"accuracy": acc, "f1": f1}


# è‡ªå®šä¹‰æ”¯æŒ R-Drop çš„ Trainer
class RDropTrainer(Trainer):
    """
    å®ç° R-Drop æŸå¤±ï¼šäº¤å‰ç†µ + KL ä¸€è‡´æ€§çº¦æŸã€‚
    æ€è·¯ï¼šåŒä¸€æ‰¹æ ·æœ¬å‰å‘ä¸¤æ¬¡ï¼Œåˆ©ç”¨ dropout çš„éšæœºæ€§è®©è¾“å‡ºåˆ†å¸ƒä¿æŒä¸€è‡´ã€‚
    """
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")

        # ç¬¬ä¸€æ¬¡å‰å‘ï¼Œè·å¾— logits1
        outputs1 = model(**inputs)
        logits1 = outputs1.logits

        # ç¬¬äºŒæ¬¡å‰å‘ï¼ˆåŒä¸€è¾“å…¥ï¼Œåˆ©ç”¨ dropout äº§ç”Ÿéšæœºæ€§ï¼‰ï¼Œè·å¾— logits2
        outputs2 = model(**inputs)
        logits2 = outputs2.logits

        # äº¤å‰ç†µä¸»æŸå¤±ï¼šä¸¤æ¬¡è¾“å‡ºåˆ†åˆ«è®¡ç®— CEï¼Œå†å–å¹³å‡
        loss_ce = 0.5 * (
            F.cross_entropy(logits1, labels)
            + F.cross_entropy(logits2, labels)
        )

        # KL ä¸€è‡´æ€§æŸå¤±ï¼šå¼ºåˆ¶ä¸¤æ¬¡è¾“å‡ºåˆ†å¸ƒæ¥è¿‘
        p_loss = F.kl_div(
            F.log_softmax(logits1, dim=-1),
            F.softmax(logits2, dim=-1),
            reduction="batchmean",
        )
        q_loss = F.kl_div(
            F.log_softmax(logits2, dim=-1),
            F.softmax(logits1, dim=-1),
            reduction="batchmean",
        )
        kl_loss = 0.5 * (p_loss + q_loss)

        alpha = 4.0  # R-Drop ç³»æ•°ï¼Œå¯è°ƒï¼Œç”¨äºæ§åˆ¶ä¸€è‡´æ€§æŸå¤±æƒé‡
        loss = loss_ce + alpha * kl_loss

        return (loss, outputs1) if return_outputs else loss


# è¾…åŠ©å‡½æ•°ï¼šæ‰“å°æ¨¡å‹å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
# è¿™ä¸ªå¾ˆé‡è¦ï¼Œå¯ä»¥ç›´è§‚åœ°çœ‹åˆ° LoRA çš„"é«˜æ•ˆ"ä½“ç°åœ¨å“ªé‡Œ
def print_trainable_parameters(model):
    """
    æ‰“å°æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    # æ‰“å°å‡ºæ¥ï¼Œè¿™ä¸ªæ•°å­—åœ¨å†™å®éªŒæŠ¥å‘Šæ—¶éå¸¸æœ‰ç”¨ï¼
    print(
        f"  å¯è®­ç»ƒå‚æ•° (trainable params): {trainable_params}"
        f" || æ€»å‚æ•° (all params): {all_param}"
        f" || å¯è®­ç»ƒæ¯”ä¾‹ (trainable %): {100 * trainable_params / all_param:.2f}%"
    )
    return trainable_params, all_param


# --- å¯è§†åŒ–å‡½æ•° ---
def plot_training_curves(trainer, experiment_name, save_path="./visualizations"):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆlosså’Œmetricsï¼‰
    """
    os.makedirs(save_path, exist_ok=True)
    
    history = trainer.state.log_history
    
    # æå–è®­ç»ƒå’Œè¯„ä¼°æŒ‡æ ‡
    train_loss = [x['loss'] for x in history if 'loss' in x and 'eval_loss' not in x]
    eval_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]
    eval_acc = [x['eval_accuracy'] for x in history if 'eval_accuracy' in x]
    eval_f1 = [x['eval_f1'] for x in history if 'eval_f1' in x]
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # Lossæ›²çº¿
    axes[0].plot(train_loss, label='è®­ç»ƒLoss', marker='o')
    if eval_loss:
        axes[0].plot(eval_loss, label='éªŒè¯Loss', marker='s')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title(f'{experiment_name} - Lossæ›²çº¿')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Accuracyæ›²çº¿
    if eval_acc:
        axes[1].plot(eval_acc, label='éªŒè¯å‡†ç¡®ç‡', marker='o', color='green')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Accuracy')
        axes[1].set_title(f'{experiment_name} - å‡†ç¡®ç‡æ›²çº¿')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
    
    # F1æ›²çº¿
    if eval_f1:
        axes[2].plot(eval_f1, label='éªŒè¯F1åˆ†æ•°', marker='o', color='orange')
        axes[2].set_xlabel('Epoch')
        axes[2].set_ylabel('F1 Score')
        axes[2].set_title(f'{experiment_name} - F1åˆ†æ•°æ›²çº¿')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    filename = os.path.join(save_path, f"{experiment_name.replace(' ', '_')}_training_curves.png")
    # ç¡®ä¿ä¿å­˜æ—¶ä½¿ç”¨æ­£ç¡®çš„ä¸­æ–‡å­—ä½“
    plt.savefig(filename, bbox_inches='tight', dpi=300)
    print(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {filename}")
    plt.close()


def plot_confusion_matrix(model, tokenizer, eval_dataset, device, experiment_name, save_path="./visualizations"):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    """
    os.makedirs(save_path, exist_ok=True)
    
    model.eval()
    all_predictions = []
    all_labels = []
    
    print(f"æ­£åœ¨è®¡ç®— {experiment_name} çš„æ··æ·†çŸ©é˜µ...")
    with torch.no_grad():
        for i in tqdm(range(min(1000, len(eval_dataset))), desc="é¢„æµ‹ä¸­"):
            sample = eval_dataset[i]
            text = sample['text']
            label = sample['label']
            
            inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            outputs = model(**inputs)
            pred = torch.argmax(outputs.logits, dim=-1).cpu().item()
            
            all_predictions.append(pred)
            all_labels.append(label)
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_predictions)
    
    # ç»˜åˆ¶
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Negative', 'Positive'],
                yticklabels=['Negative', 'Positive'])
    plt.title(f'{experiment_name} - æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()
    
    filename = os.path.join(save_path, f"{experiment_name.replace(' ', '_')}_confusion_matrix.png")
    # ç¡®ä¿ä¿å­˜æ—¶ä½¿ç”¨æ­£ç¡®çš„ä¸­æ–‡å­—ä½“
    plt.savefig(filename, bbox_inches='tight', dpi=300)
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {filename}")
    plt.close()
    
    # æ‰“å°åˆ†ç±»æŠ¥å‘Š
    print(f"\n{classification_report(all_labels, all_predictions, target_names=['Negative', 'Positive'])}")


def plot_comparison(results_dict, save_path="./visualizations"):
    """
    ç»˜åˆ¶ä¸‰ä¸ªå®éªŒçš„å¯¹æ¯”å›¾
    """
    os.makedirs(save_path, exist_ok=True)
    
    experiments = list(results_dict.keys())
    accuracies = [results_dict[exp].get('eval_accuracy', 0) for exp in experiments]
    f1_scores = [results_dict[exp].get('eval_f1', 0) for exp in experiments]
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    axes[0].bar(experiments, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
    axes[0].set_ylabel('å‡†ç¡®ç‡')
    axes[0].set_title('ä¸‰ä¸ªå®éªŒçš„å‡†ç¡®ç‡å¯¹æ¯”')
    axes[0].set_ylim([0.8, 0.95])
    axes[0].grid(True, alpha=0.3, axis='y')
    for i, v in enumerate(accuracies):
        axes[0].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom')
    
    # F1åˆ†æ•°å¯¹æ¯”
    axes[1].bar(experiments, f1_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
    axes[1].set_ylabel('F1åˆ†æ•°')
    axes[1].set_title('ä¸‰ä¸ªå®éªŒçš„F1åˆ†æ•°å¯¹æ¯”')
    axes[1].set_ylim([0.8, 0.95])
    axes[1].grid(True, alpha=0.3, axis='y')
    for i, v in enumerate(f1_scores):
        axes[1].text(i, v + 0.005, f'{v:.4f}', ha='center', va='bottom')
    
    plt.tight_layout()
    filename = os.path.join(save_path, "experiments_comparison.png")
    # ç¡®ä¿ä¿å­˜æ—¶ä½¿ç”¨æ­£ç¡®çš„ä¸­æ–‡å­—ä½“
    plt.savefig(filename, bbox_inches='tight', dpi=300)
    print(f"å®éªŒå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {filename}")
    plt.close()


# --- æ•°æ®å¢å¼ºå‡½æ•°å®šä¹‰ ---
# ç”¨äºå®éªŒä¸‰ï¼šLoRA + æ•°æ®å¢å¼º
try:
    from nltk.corpus import wordnet as wn
    from nltk import word_tokenize, pos_tag
    import nltk
    
    # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼Œé™é»˜ä¸‹è½½ï¼‰
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    try:
        nltk.data.find('taggers/averaged_perceptron_tagger')
    except LookupError:
        nltk.download('averaged_perceptron_tagger', quiet=True)
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        nltk.download('wordnet', quiet=True)
    
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("è­¦å‘Š: NLTK æœªå®‰è£…ï¼Œæ•°æ®å¢å¼ºä¸­çš„åŒä¹‰è¯æ›¿æ¢åŠŸèƒ½å°†å—é™ã€‚")
    print("   å®‰è£…å‘½ä»¤: pip install nltk")
    print("   å°†ä½¿ç”¨ç®€åŒ–çš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼ˆéšæœºåˆ é™¤ã€äº¤æ¢ç­‰ï¼‰")


def get_synonyms(word, pos=None):
    """è·å–åŒä¹‰è¯ï¼ˆç”¨äºæ•°æ®å¢å¼ºï¼‰"""
    if not NLTK_AVAILABLE:
        return []
    
    synonyms = set()
    try:
        for syn in wn.synsets(word, pos=pos):
            for lemma in syn.lemmas():
                synonym = lemma.name().replace('_', ' ').lower()
                if synonym != word.lower():
                    synonyms.add(synonym)
    except:
        pass
    
    return list(synonyms)


def synonym_replacement(text, n=3):
    """åŒä¹‰è¯æ›¿æ¢ï¼šéšæœºæ›¿æ¢nä¸ªè¯ä¸ºåŒä¹‰è¯"""
    if not NLTK_AVAILABLE or len(text.split()) <= 1:
        return text
    
    try:
        words = word_tokenize(text)
        pos_tags = pos_tag(words)
        
        # åªæ›¿æ¢åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯
        replaceable_pos = ['NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 
                          'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']
        
        replaceable_words = [(i, word, pos) for i, (word, pos) in enumerate(pos_tags) 
                            if pos in replaceable_pos and word.isalpha()]
        
        if len(replaceable_words) == 0:
            return text
        
        n = min(n, len(replaceable_words))
        selected = random.sample(replaceable_words, n)
        
        new_words = words.copy()
        for idx, word, pos in selected:
            synonyms = get_synonyms(word, pos)
            if synonyms:
                new_words[idx] = random.choice(synonyms)
        
        return ' '.join(new_words)
    except:
        return text


def random_deletion(text, p=0.1):
    """éšæœºåˆ é™¤ï¼šä»¥æ¦‚ç‡péšæœºåˆ é™¤è¯"""
    words = text.split()
    if len(words) <= 1:
        return text
    
    min_words = max(1, len(words) // 2)
    new_words = [w for w in words if random.random() > p]
    
    if len(new_words) < min_words:
        new_words = random.sample(words, min_words)
    
    return ' '.join(new_words) if new_words else text


def random_swap(text, n=3):
    """éšæœºäº¤æ¢ï¼šéšæœºäº¤æ¢nå¯¹ç›¸é‚»è¯çš„ä½ç½®"""
    words = text.split()
    if len(words) <= 1:
        return text
    
    new_words = words.copy()
    for _ in range(min(n, len(new_words) - 1)):
        idx1 = random.randint(0, len(new_words) - 2)
        idx2 = idx1 + 1
        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]
    
    return ' '.join(new_words)


def eda_augment(text, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, alpha_rd=0.1, num_aug=1, use_synonym=True):
    """
    EDA: Easy Data Augmentation
    ç»“åˆåŒä¹‰è¯æ›¿æ¢ã€éšæœºæ’å…¥ã€éšæœºäº¤æ¢ã€éšæœºåˆ é™¤
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        alpha_sr: åŒä¹‰è¯æ›¿æ¢æ¯”ä¾‹ï¼ˆä»…åœ¨ use_synonym=True æ—¶ç”Ÿæ•ˆï¼‰
        alpha_ri: éšæœºæ’å…¥æ¯”ä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼Œæš‚ä¸å®ç°ï¼‰
        alpha_rs: éšæœºäº¤æ¢æ¯”ä¾‹
        alpha_rd: éšæœºåˆ é™¤æ¯”ä¾‹
        num_aug: ç”Ÿæˆå¤šå°‘ä¸ªå¢å¼ºæ ·æœ¬ï¼ˆè¿™é‡Œç®€åŒ–ä¸º1ä¸ªï¼‰
        use_synonym: æ˜¯å¦ä½¿ç”¨åŒä¹‰è¯æ›¿æ¢ï¼ˆFalse å¯å¤§å¹…æå‡é€Ÿåº¦ï¼‰
    
    Returns:
        å¢å¼ºåçš„æ–‡æœ¬
    """
    words = text.split()
    num_words = len(words)
    
    if num_words == 0:
        return text
    
    augmented = text
    
    # åŒä¹‰è¯æ›¿æ¢ï¼ˆå¯é€‰ï¼Œè€—æ—¶æ“ä½œï¼‰
    if use_synonym and random.random() < alpha_sr and NLTK_AVAILABLE:
        n_sr = max(1, int(alpha_sr * num_words))
        augmented = synonym_replacement(augmented, n=n_sr)
    
    # éšæœºäº¤æ¢ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
    if random.random() < alpha_rs:
        n_rs = max(1, int(alpha_rs * num_words))
        augmented = random_swap(augmented, n=n_rs)
    
    # éšæœºåˆ é™¤ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
    if random.random() < alpha_rd:
        augmented = random_deletion(augmented, p=alpha_rd)
    
    return augmented


def augment_dataset_batch(examples, num_aug=2, use_synonym=True):
    """
    æ‰¹é‡æ•°æ®å¢å¼ºå‡½æ•°
    å¯¹æ¯ä¸ªæ ·æœ¬ç”Ÿæˆnum_augä¸ªå¢å¼ºç‰ˆæœ¬
    
    Args:
        examples: åŒ…å«'text'å’Œ'label'çš„å­—å…¸
        num_aug: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆå¤šå°‘ä¸ªå¢å¼ºç‰ˆæœ¬
        use_synonym: æ˜¯å¦ä½¿ç”¨åŒä¹‰è¯æ›¿æ¢ï¼ˆFalse å¯å¤§å¹…æå‡é€Ÿåº¦ï¼‰
    
    Returns:
        å¢å¼ºåçš„æ ·æœ¬ï¼ˆåŒ…å«åŸå§‹æ ·æœ¬+å¢å¼ºæ ·æœ¬ï¼‰
    """
    texts = examples['text']
    labels = examples['label']
    
    augmented_texts = []
    augmented_labels = []
    
    for text, label in zip(texts, labels):
        # æ·»åŠ åŸå§‹æ ·æœ¬
        augmented_texts.append(text)
        augmented_labels.append(label)
        
        # æ·»åŠ å¢å¼ºæ ·æœ¬
        for _ in range(num_aug):
            aug_text = eda_augment(text, num_aug=1, use_synonym=use_synonym)
            augmented_texts.append(aug_text)
            augmented_labels.append(label)
    
    return {'text': augmented_texts, 'label': augmented_labels}


print("--- 2. å…¨å±€é…ç½®å’Œè¾…åŠ©å‡½æ•°å®šä¹‰å®Œæ¯• ---\n")

# --- 3. åŠ è½½åˆ†è¯å™¨ (Tokenizer) ---
# æ— è®ºå“ªä¸ªå®éªŒï¼Œåˆ†è¯å™¨éƒ½æ˜¯ä¸€æ ·çš„
print("--- 3. åŠ è½½åˆ†è¯å™¨ ---")
# ç¡®ä¿ä½¿ç”¨å’Œæ¨¡å‹åŒ¹é…çš„åˆ†è¯å™¨
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, local_files_only=False)
    print(f"åˆ†è¯å™¨ '{MODEL_CHECKPOINT}' åŠ è½½æˆåŠŸã€‚")
except Exception as e:
    print(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {str(e)}")
    if MODEL_CHECKPOINT == MODEL_NAME:
        print(f"\næç¤ºï¼šç½‘ç»œè¿æ¥å¯èƒ½æœ‰é—®é¢˜ã€‚")
        print(f"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼š")
        print(f"  python download_model.py")
        print(f"æˆ–è€…å‚è€ƒ OFFLINE_SETUP.md æ‰‹åŠ¨ä¸‹è½½")
    raise

# åˆ›å»ºä¸€ä¸ªæ•°æ®æ•´ç†å™¨ (Data Collator)
# å®ƒä¼šå¸®æˆ‘ä»¬æŠŠä¸€ä¸ª batch é‡Œçš„æ•°æ®åŠ¨æ€ padding åˆ°ç›¸åŒçš„é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
print()

# --- 4. å‡†å¤‡æ•°æ®é›† (ç”¨äºå®éªŒä¸€å’Œå®éªŒäºŒ) ---
# è¿™æ˜¯ä¸€ä¸ªâ€œåˆå­¦è€…â€ç‰ˆæœ¬çš„é¢„å¤„ç†ï¼Œæ²¡æœ‰åšé¢å¤–çš„æ•°æ®æ¸…æ´—
print("--- 4. å‡†å¤‡æ•°æ®é›† (å®éªŒä¸€å’Œå®éªŒäºŒï¼šåŸºç¡€é¢„å¤„ç†) ---")


def tokenize_function(examples):
    # æ ¸å¿ƒï¼šä½¿ç”¨åˆ†è¯å™¨å¤„ç†æ–‡æœ¬ã€‚
    # truncation=True æ„å‘³ç€å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼ˆè¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦ï¼‰ï¼Œå°±æˆªæ–­ã€‚
    # è¿™æ˜¯å¿…é¡»çš„ï¼Œå¦åˆ™æ¨¡å‹ä¼šæŠ¥é”™ã€‚
    return tokenizer(examples["text"], truncation=True)


# åŠ è½½ IMDb æ•°æ®é›†
# æ³¨æ„ï¼šä»æœ¬åœ° parquet æ–‡ä»¶åŠ è½½ï¼Œä¸ä½¿ç”¨åœ¨çº¿ä¸‹è½½
print("--- 4. å‡†å¤‡æ•°æ®é›† (ä»æœ¬åœ° parquet æ–‡ä»¶åŠ è½½) ---")

# æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰ parquet æ–‡ä»¶
imdb_local_path = os.path.join(LOCAL_DATASET_CACHE, "imdb")
train_parquet = os.path.join(imdb_local_path, "train-00000-of-00001.parquet")
test_parquet = os.path.join(imdb_local_path, "test-00000-of-00001.parquet")

if not os.path.exists(train_parquet) or not os.path.exists(test_parquet):
    print("æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®é›†æ–‡ä»¶")
    print(f"   è¯·ç¡®ä¿æ•°æ®é›†æ–‡ä»¶å­˜åœ¨äº: {imdb_local_path}")
    print(f"   éœ€è¦çš„æ–‡ä»¶:")
    print(f"     - train-00000-of-00001.parquet")
    print(f"     - test-00000-of-00001.parquet")
    raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {imdb_local_path}")

# ä»æœ¬åœ° parquet æ–‡ä»¶åŠ è½½
print(f"--- ä»æœ¬åœ° parquet æ–‡ä»¶åŠ è½½æ•°æ®é›† ---")
print(f"   æ•°æ®è·¯å¾„: {imdb_local_path}")

# æ„å»ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆæ”¯æŒå¤šä¸ªåˆ†ç‰‡ï¼‰
train_files = glob.glob(os.path.join(imdb_local_path, "train-*.parquet"))
test_files = glob.glob(os.path.join(imdb_local_path, "test-*.parquet"))

if not train_files or not test_files:
    raise FileNotFoundError(f"æœªæ‰¾åˆ°å®Œæ•´çš„ parquet æ–‡ä»¶ï¼ˆéœ€è¦ train å’Œ test æ–‡ä»¶ï¼‰")

try:
    raw_datasets = load_dataset(
        "parquet",
        data_files={
            "train": train_files,
            "test": test_files
        }
    )
    print("ä»æœ¬åœ° parquet æ–‡ä»¶åŠ è½½æˆåŠŸ")
    print(f"   è®­ç»ƒé›†æ–‡ä»¶æ•°: {len(train_files)}, æµ‹è¯•é›†æ–‡ä»¶æ•°: {len(test_files)}")
except Exception as e:
    print(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {str(e)}")
    print(f"\næç¤ºï¼šè¯·ç¡®ä¿å·²å®‰è£… datasets åº“: pip install datasets")
    raise

# ä½¿ç”¨æ›´å¤šçš„è®­ç»ƒæ•°æ®ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ
train_dataset = raw_datasets["train"].shuffle(seed=42).select(range(TRAIN_SAMPLES))
eval_dataset = raw_datasets["test"].shuffle(seed=42).select(range(EVAL_SAMPLES))

print(f"åŸå§‹æ•°æ®é›†åŠ è½½æˆåŠŸ: {raw_datasets}")
print(f"ç”¨äºå®éªŒçš„æ ·æœ¬é‡: è®­ç»ƒé›† {len(train_dataset)}, æµ‹è¯•é›† {len(eval_dataset)}")

# ä½¿ç”¨ .map() æ–¹æ³•æ‰¹é‡å¤„ç†æ•°æ®é›†
# batched=True å¯ä»¥è®©åˆ†è¯å™¨ä¸€æ¬¡å¤„ç†ä¸€æ‰¹æ•°æ®ï¼Œé€Ÿåº¦æ›´å¿«
tokenized_datasets_basic = raw_datasets.map(tokenize_function, batched=True)
# å‡†å¤‡ç”¨äºè®­ç»ƒå™¨çš„æ•°æ®é›†
train_dataset_basic = tokenized_datasets_basic["train"].shuffle(seed=42).select(range(TRAIN_SAMPLES))
eval_dataset_basic = tokenized_datasets_basic["test"].shuffle(seed=42).select(range(EVAL_SAMPLES))

print("åŸºç¡€é¢„å¤„ç†ï¼ˆä»…åˆ†è¯ï¼‰å®Œæˆã€‚\n")

# --- 5. å®éªŒä¸€ (Baseline): å…¨é‡å‚æ•°å¾®è°ƒ ---
print("==============================================")
print("--- 5. å¼€å§‹å®éªŒä¸€ (Baseline): å…¨é‡å‚æ•°å¾®è°ƒ ---")
print("==============================================")

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
# æ€è€ƒï¼šè¿™é‡Œæˆ‘ä»¬åŠ è½½çš„æ˜¯ DistilBertForSequenceClassification
# num_labels=2 å‘Šè¯‰æ¨¡å‹è¿™æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ (IMDb: ç§¯æ/æ¶ˆæ)
try:
    model_baseline = AutoModelForSequenceClassification.from_pretrained(
        MODEL_CHECKPOINT,
        num_labels=2,
        local_files_only=False
    )
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    if MODEL_CHECKPOINT == MODEL_NAME:
        print(f"\næç¤ºï¼šç½‘ç»œè¿æ¥å¯èƒ½æœ‰é—®é¢˜ã€‚")
        print(f"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼š")
        print(f"  python download_model.py")
        print(f"æˆ–è€…å‚è€ƒ OFFLINE_SETUP.md æ‰‹åŠ¨ä¸‹è½½")
    raise

# æ‰“å°ä¸€ä¸‹æ¨¡å‹å‚æ•°ï¼Œçœ‹çœ‹å…¨é‡å¾®è°ƒè¦åŠ¨å¤šå°‘å‚æ•°
print("æ¨¡å‹ (Baseline) å¯è®­ç»ƒå‚æ•°:")
print_trainable_parameters(model_baseline)

# å®šä¹‰è®­ç»ƒå‚æ•°
# æ€è€ƒï¼šoutput_dir æ˜¯å­˜æ”¾æ¨¡å‹å’Œæ—¥å¿—çš„åœ°æ–¹ï¼Œæ¯ä¸ªå®éªŒç”¨ä¸åŒçš„ç›®å½•
training_args_baseline = TrainingArguments(
    output_dir="./results/baseline",
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE_BASELINE,  # Baseline ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
    weight_decay=0.01,
    eval_strategy="epoch",  # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    save_strategy="epoch",  # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡
    load_best_model_at_end=True,  # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€å¥½çš„æ¨¡å‹
    metric_for_best_model="eval_f1",  # ä½¿ç”¨ F1 ä½œä¸ºæœ€ä½³æ¨¡å‹æŒ‡æ ‡
    greater_is_better=True,  # F1 è¶Šå¤§è¶Šå¥½
    save_total_limit=3,  # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpointï¼ŒèŠ‚çœç©ºé—´
    report_to="none",  # ç¦ç”¨ wandb å’Œ TensorBoardï¼ˆé¿å…ç½‘ç»œå’Œè·¯å¾„é—®é¢˜ï¼‰
    logging_dir=None,  # ç¦ç”¨ TensorBoard æ—¥å¿—
    run_name="baseline",  # è®¾ç½®ç®€å•çš„è¿è¡Œåç§°
    warmup_steps=100,  # æ·»åŠ warmupï¼Œå¸®åŠ©è®­ç»ƒç¨³å®š
    logging_steps=100,  # æ¯100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    disable_tqdm=False,  # ä¸ç¦ç”¨tqdmï¼Œä½†æˆ‘ä»¬ä¼šé€šè¿‡è‡ªå®šä¹‰å›è°ƒæ¥æ§åˆ¶
    logging_first_step=True,  # è®°å½•ç¬¬ä¸€æ­¥
)

# åˆ›å»ºè®­ç»ƒå™¨ Trainer
trainer_baseline = Trainer(
    model=model_baseline,
    args=training_args_baseline,
    train_dataset=train_dataset_basic,
    eval_dataset=eval_dataset_basic,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,  # ä¼ å…¥æˆ‘ä»¬å®šä¹‰çš„è¯„ä¼°å‡½æ•°
)

# å¼€å§‹è®­ç»ƒï¼
print("\n--- å¼€å§‹è®­ç»ƒ (Baseline) ---")
trainer_baseline.train()
print("--- è®­ç»ƒ (Baseline) å®Œæˆ ---")

# è¯„ä¼°æ¨¡å‹
print("\n--- å¼€å§‹è¯„ä¼° (Baseline) ---")
eval_results_baseline = trainer_baseline.evaluate()
print("è¯„ä¼°ç»“æœ (Baseline):", eval_results_baseline)

# ç”Ÿæˆå¯è§†åŒ–
print("\n--- ç”Ÿæˆå¯è§†åŒ– (Baseline) ---")
plot_training_curves(trainer_baseline, "å®éªŒä¸€_Baseline")
plot_confusion_matrix(model_baseline, tokenizer, eval_dataset_basic, 
                     torch.device("cuda" if torch.cuda.is_available() else "cpu"),
                     "å®éªŒä¸€_Baseline")

# ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨ç”¨äºå‰ç«¯
print("\n--- ä¿å­˜æ¨¡å‹ (Baseline) ç”¨äºå‰ç«¯ ---")
model_save_path_baseline = "./saved_models/baseline"
model_baseline.save_pretrained(model_save_path_baseline)
tokenizer.save_pretrained(model_save_path_baseline)
print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path_baseline}")

print("--- å®éªŒä¸€ (Baseline) ç»“æŸ ---\n")

# --- 6. å®éªŒäºŒ (æ ¸å¿ƒè¦æ±‚): LoRA é«˜æ•ˆå¾®è°ƒ ---
print("==============================================")
print("--- 6. å¼€å§‹å®éªŒäºŒ (æ ¸å¿ƒè¦æ±‚): LoRA é«˜æ•ˆå¾®è°ƒ ---")
print("==============================================")

# æ€è€ƒï¼šå®éªŒè¦æ±‚ä½¿ç”¨ LoRAã€‚æˆ‘éœ€è¦å…ˆåŠ è½½ä¸€ä¸ª *æ–°* çš„ã€*æœªè®­ç»ƒ* çš„æ¨¡å‹ã€‚
# ä¸èƒ½ä½¿ç”¨ä¸Šä¸€ä¸ªå®éªŒè®­ç»ƒè¿‡çš„ `model_baseline`ã€‚
try:
    model_lora = AutoModelForSequenceClassification.from_pretrained(
        MODEL_CHECKPOINT,
        num_labels=2,
        local_files_only=False
    )
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    if MODEL_CHECKPOINT == MODEL_NAME:
        print(f"\næç¤ºï¼šç½‘ç»œè¿æ¥å¯èƒ½æœ‰é—®é¢˜ã€‚")
        print(f"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼š")
        print(f"  python download_model.py")
    raise

# æ€è€ƒï¼šæ¥ä¸‹æ¥æ˜¯ LoRA çš„æ ¸å¿ƒé…ç½®ã€‚
# æˆ‘éœ€è¦å®šä¹‰ LoraConfigã€‚
# ä¼˜åŒ–é…ç½®ï¼šå¢åŠ  r å€¼ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ
# 1. `r`: LoRA çš„ç§©ã€‚ä» 8 å¢åŠ åˆ° 16ï¼Œå¢åŠ å‚æ•°é‡ä»¥æå‡æ•ˆæœã€‚
# 2. `lora_alpha`: LoRA çš„ç¼©æ”¾å› å­ï¼Œé€šå¸¸æ˜¯ `r` çš„ä¸¤å€ï¼ˆ16 -> 32ï¼‰ã€‚
# 3. `target_modules`: è¦åœ¨å“ªäº›å±‚åº”ç”¨ LoRAã€‚
#    - DistilBERT çš„ Transformer å—é‡Œæœ‰ 'q_lin' (query) å’Œ 'v_lin' (value)ã€‚
#    - å¯ä»¥æ‰©å±•åˆ°æ›´å¤šå±‚ï¼š'k_lin' (key) å’Œ 'out_lin' (output)
# 4. `task_type`: å¿…é¡»æŒ‡å®šï¼Œæˆ‘ä»¬è¿™æ˜¯åºåˆ—åˆ†ç±»ã€‚
# ä¼˜åŒ–LoRAé…ç½®ä»¥å°è¯•è¶…è¶Šbaseline
# ç­–ç•¥ï¼šå¢åŠ rå€¼åˆ°32ï¼Œå¢åŠ å‚æ•°é‡ä»¥æå‡è¡¨è¾¾èƒ½åŠ›
lora_config = LoraConfig(
    r=32,  # ä»16å¢åŠ åˆ°32ï¼Œå¤§å¹…å¢åŠ å‚æ•°é‡ä»¥æå‡æ•ˆæœï¼ˆå°è¯•è¶…è¶Šbaselineï¼‰
    lora_alpha=64,  # ç›¸åº”å¢åŠ åˆ°64ï¼ˆrçš„ä¸¤å€ï¼Œä¿æŒæ¯”ä¾‹ï¼‰
    target_modules=["q_lin", "v_lin", "k_lin", "out_lin"],  # æ‰©å±•åˆ°æ›´å¤šå±‚ï¼šQuery, Value, Key, Output
    lora_dropout=0.05,  # é™ä½dropoutä»¥æå‡æ¨¡å‹å®¹é‡
    bias="none",  # "none" æ˜¯ä¸€ä¸ªå¸¸è§çš„è®¾ç½®
    task_type=TaskType.SEQ_CLS,  # ä»»åŠ¡ç±»å‹ï¼šåºåˆ—åˆ†ç±»
)

# ä½¿ç”¨ `get_peft_model` æ¥åŒ…è£…æˆ‘ä»¬çš„åŸºç¡€æ¨¡å‹
model_lora_peft = get_peft_model(model_lora, lora_config)

# å…³é”®ä¸€æ­¥ï¼šæ‰“å° LoRA æ¨¡å‹çš„å‚æ•°ï¼
# è¿™å°†æ˜¯å®éªŒæŠ¥å‘Šçš„äº®ç‚¹ï¼Œå¯¹æ¯”å®éªŒä¸€çš„å‚æ•°é‡ã€‚
print("æ¨¡å‹ (LoRA) å¯è®­ç»ƒå‚æ•°:")
print_trainable_parameters(model_lora_peft)

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args_lora = TrainingArguments(
    output_dir="./results/lora_basic",  # æ¢ä¸ªç›®å½•
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE_LORA,  # LoRA ä½¿ç”¨ä¼˜åŒ–çš„å­¦ä¹ ç‡ï¼ˆ3e-5ï¼‰
    weight_decay=0.01,
    eval_strategy="epoch",  # ä½¿ç”¨æ–°å‚æ•°å
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_f1",  # ä½¿ç”¨ F1 ä½œä¸ºæœ€ä½³æ¨¡å‹æŒ‡æ ‡
    greater_is_better=True,
    save_total_limit=3,  # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpoint
    report_to="none",  # ç¦ç”¨ wandb å’Œ TensorBoard
    logging_dir=None,  # ç¦ç”¨ TensorBoard æ—¥å¿—
    run_name="lora_basic",  # è®¾ç½®ç®€å•çš„è¿è¡Œåç§°
    warmup_steps=100,  # æ·»åŠ warmup
    logging_steps=100,  # æ¯100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    disable_tqdm=False,  # ä¸ç¦ç”¨tqdm
    logging_first_step=True,
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer_lora = Trainer(
    model=model_lora_peft,  # æ³¨æ„ï¼šè¿™é‡Œç”¨çš„æ˜¯ PEFT åŒ…è£…è¿‡çš„æ¨¡å‹
    args=training_args_lora,
    train_dataset=train_dataset_basic,
    eval_dataset=eval_dataset_basic,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# å¼€å§‹è®­ç»ƒï¼
print("\n--- å¼€å§‹è®­ç»ƒ (LoRA) ---")
trainer_lora.train()
print("--- è®­ç»ƒ (LoRA) å®Œæˆ ---")

# è¯„ä¼°æ¨¡å‹
print("\n--- å¼€å§‹è¯„ä¼° (LoRA) ---")
eval_results_lora = trainer_lora.evaluate()
print("è¯„ä¼°ç»“æœ (LoRA):", eval_results_lora)

# ç”Ÿæˆå¯è§†åŒ–
print("\n--- ç”Ÿæˆå¯è§†åŒ– (LoRA) ---")
plot_training_curves(trainer_lora, "å®éªŒäºŒ_LoRA")
plot_confusion_matrix(trainer_lora.model, tokenizer, eval_dataset_basic,
                     torch.device("cuda" if torch.cuda.is_available() else "cpu"),
                     "å®éªŒäºŒ_LoRA")

# ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨ç”¨äºå‰ç«¯
print("\n--- ä¿å­˜æ¨¡å‹ (LoRA) ç”¨äºå‰ç«¯ ---")
model_save_path_lora = "./saved_models/lora_basic"
# å¯¹äº PEFT æ¨¡å‹ï¼Œéœ€è¦ä¿å­˜ PEFT é€‚é…å™¨å’ŒåŸºç¡€æ¨¡å‹
trainer_lora.model.save_pretrained(model_save_path_lora)
tokenizer.save_pretrained(model_save_path_lora)
print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path_lora}")

print("--- å®éªŒäºŒ (LoRA) ç»“æŸ ---\n")

# --- 7. å®éªŒä¸‰ (æ”¹è¿›å·¥ä½œ): LoRA + é¢†åŸŸé€‚åº”æ€§é¢„è®­ç»ƒ (DAPT) ---
# æ€è€ƒï¼šè¿™æ˜¯æœ€é€‚åˆä½œä¸º"æ”¹è¿›ç‚¹"çš„æ–¹å‘ï¼Œé€»è¾‘é€šé¡ºä¸”æ•ˆæœç«‹ç«¿è§å½±ã€‚
# 
# åŸç†ï¼šDistilBERT æ˜¯åœ¨ Wikipediaï¼ˆç»´åŸºç™¾ç§‘ï¼‰ä¸Šé¢„è®­ç»ƒçš„ï¼Œé‚£æ˜¯"ç™¾ç§‘å…¨ä¹¦å¼çš„è‹±è¯­"ã€‚
#      IMDb æ˜¯ç”µå½±è¯„è®ºï¼Œå……æ»¡äº†å£è¯­ã€ä¿šè¯­ã€æ¿€çƒˆçš„æƒ…ç»ªè¯ï¼Œè¿™æ˜¯"å½±è¯„å¼çš„è‹±è¯­"ã€‚
#      æ¨¡å‹è¿˜æ²¡å­¦ä¼š"è¯»å½±è¯„"ï¼Œå°±é€¼å®ƒ"åšåˆ†ç±»"ï¼Œæ•ˆæœè‚¯å®šæ‰“æŠ˜ã€‚
# 
# æ–¹æ³•ï¼šé¢†åŸŸé€‚åº”æ€§é¢„è®­ç»ƒ (Domain-Adaptive Pre-training, DAPT)
#      ç¬¬ä¸€é˜¶æ®µ (DAPT)ï¼šåœ¨ IMDb çš„æ‰€æœ‰æ–‡æœ¬ä¸Šç»§ç»­åš MLM (Masked Language Modeling) é¢„è®­ç»ƒ
#      ç¬¬äºŒé˜¶æ®µ (Fine-tuning)ï¼šåœ¨ DAPT åçš„æ¨¡å‹ä¸Šæ¥åˆ†ç±»å±‚ï¼Œåš LoRA å¾®è°ƒ
# 
# ä¸ºä»€ä¹ˆæ–°é¢–ï¼šè¿™å«"ç»§ç»­é¢„è®­ç»ƒ"(Continued Pre-training)ï¼Œåœ¨å­¦æœ¯ç•Œæ˜¯å…¬è®¤æœ‰æ•ˆçš„æå‡æ‰‹æ®µ
#            (ACL 2020 è®ºæ–‡ "Don't Stop Pretraining")
# 
# é¢„æœŸæå‡ï¼šé€šå¸¸èƒ½ç¨³å®šæå‡ 0.5% - 1.5%ï¼Œæ¯”æ•°æ®å¢å¼ºæ›´æœ‰æ•ˆä¸”æ›´åˆç†
# 
#    å…³äºæ•°æ®å¢å¼ºçš„è¯´æ˜ï¼š
#     æ•°æ®å¢å¼ºï¼ˆEDAï¼‰è™½ç„¶ä¹Ÿèƒ½æå‡æ•ˆæœï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
#     1. è€—æ—¶å¤§ï¼šåŒä¹‰è¯æ›¿æ¢éœ€è¦ NLTK è¯æ€§æ ‡æ³¨å’Œ WordNet æŸ¥è¯¢ï¼Œå¯¹ 10000 æ¡æ ·æœ¬
#        åš 2 å€å¢å¼ºï¼ˆå…± 30000 æ¡ï¼‰å¯èƒ½éœ€è¦æ•°å°æ—¶
#     2. æå‡å°‘ï¼šé¢„è®­ç»ƒæ¨¡å‹ï¼ˆDistilBERTï¼‰å·²ç»å¾ˆå¼ºï¼Œæ•°æ®å¢å¼ºçš„è¾¹é™…æ”¶ç›Šé€šå¸¸åªæœ‰ 0.5-1%ï¼Œ
#        ä¸å¦‚ DAPT è¿™ç§é¢†åŸŸé€‚åº”æ–¹æ³•æ˜æ˜¾
#     3. é£é™©é«˜ï¼šåŒä¹‰è¯æ›¿æ¢å¯èƒ½æ”¹å˜æƒ…æ„Ÿå¼ºåº¦ï¼ˆå¦‚"good"å’Œ"great"è™½ç„¶åŒä¹‰ä½†å¼ºåº¦ä¸åŒï¼‰ï¼Œ
#        åœ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­å¯èƒ½å¼•å…¥å™ªéŸ³
print("==============================================")
print("--- 7. å¼€å§‹å®éªŒä¸‰ (æ”¹è¿›): LoRA + DAPT (é¢†åŸŸé€‚åº”æ€§é¢„è®­ç»ƒ) ---")
print("==============================================")

# ========== é˜¶æ®µ1: DAPT (é¢†åŸŸé€‚åº”æ€§é¢„è®­ç»ƒ) ==========
# åœ¨ IMDb æ•°æ®ä¸Šç»§ç»­åš MLM é¢„è®­ç»ƒï¼Œè®©æ¨¡å‹é€‚åº”å½±è¯„é¢†åŸŸçš„è¯­è¨€é£æ ¼
print("\n--- é˜¶æ®µ1: DAPT (é¢†åŸŸé€‚åº”æ€§é¢„è®­ç»ƒ) ---")
print("ç›®æ ‡ï¼šè®© DistilBERT é€‚åº” IMDb å½±è¯„é¢†åŸŸçš„è¯­è¨€é£æ ¼ï¼ˆå£è¯­ã€ä¿šè¯­ã€æƒ…ç»ªè¯ï¼‰")

# 1. å‡†å¤‡ MLM é¢„è®­ç»ƒæ•°æ®
# ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ IMDb æ–‡æœ¬ï¼ˆtrain + testï¼‰ï¼Œä¸éœ€è¦æ ‡ç­¾
print("\nå‡†å¤‡ MLM é¢„è®­ç»ƒæ•°æ®...")
# åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–‡æœ¬ï¼ˆç”¨äºæ— ç›‘ç£é¢„è®­ç»ƒï¼‰
dapt_texts_train = raw_datasets["train"]["text"]
dapt_texts_test = raw_datasets["test"]["text"]
all_dapt_texts = dapt_texts_train + dapt_texts_test
print(f"   æ€»æ–‡æœ¬æ•°: {len(all_dapt_texts)}")
print(f"   ä½¿ç”¨å‰ {min(20000, len(all_dapt_texts))} æ¡æ–‡æœ¬è¿›è¡Œ DAPTï¼ˆå¯è°ƒæ•´ï¼‰")

# é™åˆ¶ DAPT æ•°æ®é‡ä»¥èŠ‚çœæ—¶é—´ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
DAPT_SAMPLES = min(20000, len(all_dapt_texts))
dapt_texts = all_dapt_texts[:DAPT_SAMPLES]

# åˆ›å»ºç”¨äº MLM çš„æ•°æ®é›†
dapt_dataset = Dataset.from_dict({"text": dapt_texts})

# 2. å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼ˆç”¨äº MLMï¼‰
def tokenize_function_mlm(examples):
    """å¯¹ MLM é¢„è®­ç»ƒçš„æ–‡æœ¬è¿›è¡Œåˆ†è¯"""
    return tokenizer(examples["text"], truncation=True, max_length=512)

print("å¯¹ DAPT æ•°æ®è¿›è¡Œåˆ†è¯...")
dapt_dataset_tokenized = dapt_dataset.map(
    tokenize_function_mlm,
    batched=True,
    remove_columns=["text"],  # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—ï¼Œåªä¿ç•™ tokenized ç»“æœ
    desc="åˆ†è¯ä¸­..."
)

# 3. åˆ›å»º MLM æ•°æ®æ•´ç†å™¨
# MLM éœ€è¦éšæœº mask ä¸€äº› tokenï¼ŒDataCollatorForLanguageModeling ä¼šè‡ªåŠ¨å¤„ç†
mlm_probability = 0.15  # 15% çš„ token ä¼šè¢« maskï¼ˆBERT æ ‡å‡†ï¼‰
data_collator_mlm = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=True,  # å¯ç”¨ MLM
    mlm_probability=mlm_probability
)

# 4. åŠ è½½ MLM æ¨¡å‹ï¼ˆç”¨äºé¢„è®­ç»ƒï¼‰
print("\nåŠ è½½ MLM æ¨¡å‹ç”¨äº DAPT...")
try:
    model_dapt_mlm = AutoModelForMaskedLM.from_pretrained(
        MODEL_CHECKPOINT,
        local_files_only=False
    )
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    if MODEL_CHECKPOINT == MODEL_NAME:
        print(f"\næç¤ºï¼šç½‘ç»œè¿æ¥å¯èƒ½æœ‰é—®é¢˜ã€‚")
        print(f"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼š")
        print(f"  python download_model.py")
    raise

print("æ¨¡å‹ (DAPT MLM) å¯è®­ç»ƒå‚æ•°:")
print_trainable_parameters(model_dapt_mlm)  # å…¨é‡å‚æ•°å¯è®­ç»ƒ

# 5. é…ç½® DAPT è®­ç»ƒå‚æ•°
# DAPT é€šå¸¸åªéœ€è¦ 1-3 ä¸ª epochï¼Œå­¦ä¹ ç‡å¯ä»¥ç¨å¤§
DAPT_EPOCHS = 2  # DAPT é¢„è®­ç»ƒè½®æ¬¡ï¼ˆé€šå¸¸ 1-3 è½®å³å¯ï¼‰
training_args_dapt = TrainingArguments(
    output_dir="./results/dapt_mlm",
    num_train_epochs=DAPT_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE_DAPT_MLM,  # DAPT ä½¿ç”¨ç¨å¤§çš„å­¦ä¹ ç‡
    weight_decay=0.01,
    save_strategy="epoch",
    save_total_limit=1,  # åªä¿ç•™æœ€åä¸€ä¸ª checkpoint
    report_to="none",
    logging_dir=None,
    run_name="dapt_mlm",
    warmup_steps=100,
    logging_steps=200,
    disable_tqdm=False,
    logging_first_step=True,
)

# 6. åˆ›å»º DAPT Trainerï¼ˆMLM ä»»åŠ¡ä¸éœ€è¦ compute_metricsï¼‰
trainer_dapt = Trainer(
    model=model_dapt_mlm,
    args=training_args_dapt,
    train_dataset=dapt_dataset_tokenized,
    tokenizer=tokenizer,
    data_collator=data_collator_mlm,
)

# 7. å¼€å§‹ DAPT é¢„è®­ç»ƒ
print(f"\n--- å¼€å§‹ DAPT é¢„è®­ç»ƒ ({DAPT_EPOCHS} ä¸ª epoch) ---")
print("æç¤ºï¼šè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œä½†æ¯”æ•°æ®å¢å¼ºå¿«å¾—å¤š...")
trainer_dapt.train()
print("--- DAPT é¢„è®­ç»ƒå®Œæˆ ---")

# 8. ä¿å­˜ DAPT åçš„æ¨¡å‹
dapt_model_path = "./saved_models/dapt_base"
print(f"\nä¿å­˜ DAPT åçš„æ¨¡å‹åˆ°: {dapt_model_path}")
trainer_dapt.model.save_pretrained(dapt_model_path)
tokenizer.save_pretrained(dapt_model_path)
print("DAPT æ¨¡å‹ä¿å­˜å®Œæˆ")

# ========== é˜¶æ®µ2: åœ¨ DAPT åçš„æ¨¡å‹ä¸Šåš LoRA å¾®è°ƒ ==========
print("\n--- é˜¶æ®µ2: åœ¨ DAPT åçš„æ¨¡å‹ä¸Šåš LoRA å¾®è°ƒ ---")
print("ç›®æ ‡ï¼šåœ¨é¢†åŸŸé€‚åº”çš„æ¨¡å‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨ LoRA è¿›è¡Œé«˜æ•ˆå¾®è°ƒ")

# 1. å‡†å¤‡åˆ†ç±»ä»»åŠ¡æ•°æ®ï¼ˆä½¿ç”¨åŸºç¡€æ•°æ®é›†ï¼Œä¸éœ€è¦æ•°æ®å¢å¼ºï¼‰
print("\nå‡†å¤‡åˆ†ç±»ä»»åŠ¡æ•°æ®...")
train_dataset_raw = raw_datasets["train"].shuffle(seed=42).select(range(TRAIN_SAMPLES))
eval_dataset_raw = raw_datasets["test"].shuffle(seed=42).select(range(EVAL_SAMPLES))

# å¯¹æ•°æ®è¿›è¡Œåˆ†è¯
train_dataset_advanced = train_dataset_raw.map(tokenize_function, batched=True)
eval_dataset_advanced = eval_dataset_raw.map(tokenize_function, batched=True)
print("æ•°æ®é¢„å¤„ç†å®Œæˆ")

# 2. ä» DAPT åçš„æ¨¡å‹åŠ è½½ï¼Œåˆ›å»ºåˆ†ç±»æ¨¡å‹
print("\nä» DAPT åçš„æ¨¡å‹åˆ›å»ºåˆ†ç±»æ¨¡å‹...")
try:
    # å…ˆåŠ è½½ DAPT åçš„ MLM æ¨¡å‹
    model_dapt_mlm_loaded = AutoModelForMaskedLM.from_pretrained(
        dapt_model_path,
        local_files_only=False
    )
    
    # ä» MLM æ¨¡å‹æå– DistilBERT çš„ encoder éƒ¨åˆ†ï¼Œåˆ›å»ºåˆ†ç±»æ¨¡å‹
    # æ³¨æ„ï¼šDistilBERT çš„ MLM å’Œåˆ†ç±»æ¨¡å‹å…±äº« encoderï¼Œå¯ä»¥ç›´æ¥è½¬æ¢
    model_lora_advanced = AutoModelForSequenceClassification.from_pretrained(
        dapt_model_path,  # ä» DAPT æ¨¡å‹åŠ è½½
        num_labels=2,
        local_files_only=False
    )
    
    # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•ä»åŸºç¡€æ¨¡å‹åŠ è½½å¹¶å¤åˆ¶ DAPT çš„æƒé‡
    # ï¼ˆè¿™éœ€è¦æ‰‹åŠ¨å¤„ç†ï¼Œä½† transformers é€šå¸¸ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
except Exception as e:
    print(f"ä» DAPT æ¨¡å‹åˆ›å»ºåˆ†ç±»æ¨¡å‹å¤±è´¥: {str(e)}")
    print("å°è¯•ä»åŸºç¡€æ¨¡å‹åŠ è½½å¹¶æ‰‹åŠ¨å¤åˆ¶ DAPT æƒé‡...")
    # å¤‡ç”¨æ–¹æ¡ˆï¼šä»åŸºç¡€æ¨¡å‹åŠ è½½ï¼Œç„¶åæ‰‹åŠ¨å¤åˆ¶ encoder æƒé‡
    model_lora_advanced = AutoModelForSequenceClassification.from_pretrained(
        MODEL_CHECKPOINT,
        num_labels=2,
        local_files_only=False
    )
    # åŠ è½½ DAPT çš„ encoder æƒé‡
    dapt_state_dict = model_dapt_mlm_loaded.distilbert.state_dict()
    model_lora_advanced.distilbert.load_state_dict(dapt_state_dict)
    print("å·²æ‰‹åŠ¨å¤åˆ¶ DAPT æƒé‡åˆ°åˆ†ç±»æ¨¡å‹")

# 3. åº”ç”¨ LoRAï¼ˆä½¿ç”¨å’Œå®éªŒäºŒå®Œå…¨ç›¸åŒçš„é…ç½®ï¼‰
model_lora_advanced_peft = get_peft_model(model_lora_advanced, lora_config)

print("æ¨¡å‹ (LoRA + DAPT) å¯è®­ç»ƒå‚æ•°:")
print_trainable_parameters(model_lora_advanced_peft)  # å‚æ•°é‡åº”è¯¥å’Œå®éªŒäºŒä¸€æ ·

# 4. å‡†å¤‡è®­ç»ƒå‚æ•°
training_args_lora_advanced = TrainingArguments(
    output_dir="./results/lora_advanced",
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE_DAPT_FINETUNE,  # DAPT åçš„å¾®è°ƒä½¿ç”¨æ ‡å‡†å­¦ä¹ ç‡
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_f1",
    greater_is_better=True,
    save_total_limit=3,
    report_to="none",
    logging_dir=None,
    run_name="lora_dapt",
    warmup_steps=100,
    logging_steps=100,
    disable_tqdm=False,
    logging_first_step=True,
)

trainer_lora_advanced = Trainer(
    model=model_lora_advanced_peft,
    args=training_args_lora_advanced,
    train_dataset=train_dataset_advanced,
    eval_dataset=eval_dataset_advanced,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# 5. å¼€å§‹è®­ç»ƒ
print("\n--- å¼€å§‹è®­ç»ƒ (LoRA + DAPT) ---")
trainer_lora_advanced.train()
print("--- è®­ç»ƒ (LoRA + DAPT) å®Œæˆ ---")

# 6. è¯„ä¼°æ¨¡å‹
print("\n--- å¼€å§‹è¯„ä¼° (LoRA + DAPT) ---")
eval_results_lora_advanced = trainer_lora_advanced.evaluate()
print("è¯„ä¼°ç»“æœ (LoRA + DAPT):", eval_results_lora_advanced)

# ç”Ÿæˆå¯è§†åŒ–
print("\n--- ç”Ÿæˆå¯è§†åŒ– (LoRA + DAPT) ---")
plot_training_curves(trainer_lora_advanced, "å®éªŒä¸‰_LoRA_DAPT")
plot_confusion_matrix(trainer_lora_advanced.model, tokenizer, eval_dataset_advanced,
                     torch.device("cuda" if torch.cuda.is_available() else "cpu"),
                     "å®éªŒä¸‰_LoRA_DAPT")

# ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨ç”¨äºå‰ç«¯
print("\n--- ä¿å­˜æ¨¡å‹ (LoRA + DAPT) ç”¨äºå‰ç«¯ ---")
model_save_path_lora_advanced = "./saved_models/lora_advanced"
trainer_lora_advanced.model.save_pretrained(model_save_path_lora_advanced)
tokenizer.save_pretrained(model_save_path_lora_advanced)
print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path_lora_advanced}")

print("--- å®éªŒä¸‰ (æ”¹è¿›) ç»“æŸ ---\n")

# --- 8. å®éªŒå››ï¼šLoRA + DAPT + R-Drop ---
# 
# ğŸ’¡ å®éªŒè®¾è®¡è¯´æ˜ï¼š
# 1. R-Drop vs DAPTï¼šR-Drop æ˜¯ç®—æ³•å±‚é¢çš„æ”¹è¿›ï¼ˆæŸå¤±å‡½æ•°æ­£åˆ™åŒ–ï¼‰ï¼Œé€šå¸¸èƒ½å¸¦æ¥é¢å¤–æå‡
#    ï¼ˆé¢„æœŸæå‡ 1-2%ï¼‰ï¼Œä¸”ä¸å¢åŠ è®­ç»ƒæ•°æ®é‡ï¼Œè®­ç»ƒæ—¶é—´ä¸å®éªŒäºŒç›¸è¿‘ã€‚
# 2. ä¸ºä»€ä¹ˆ LoRA è®­ç»ƒæ—¶é—´ä¼˜åŠ¿ä¸æ˜æ˜¾ï¼Ÿ
#    - DistilBERT åªæœ‰ 66M å‚æ•°ï¼Œå…¨é‡å¾®è°ƒä¹Ÿä¸æ…¢ï¼ˆ26åˆ†é’Ÿï¼‰
#    - LoRA çš„ä¼˜åŠ¿åœ¨æ›´å¤§æ¨¡å‹ï¼ˆå¦‚ BERT-base 110M+ã€GPT ç­‰ï¼‰ä¸Šæ›´æ˜æ˜¾
#    - å½“å‰è§„æ¨¡ä¸‹ï¼ŒLoRA çš„ä¸»è¦ä¼˜åŠ¿æ˜¯å‚æ•°é‡å°‘ï¼ˆ1.73%ï¼‰ï¼Œè€Œéè®­ç»ƒé€Ÿåº¦
# 3. æ€§èƒ½å›ç¼©é—®é¢˜ï¼š
#    - Baseline 0.93 â†’ 0.91ï¼šå¯èƒ½æ˜¯å­¦ä¹ ç‡/epoch æ•°è°ƒæ•´å¯¼è‡´
#    - LoRA 0.91ï¼šåœ¨å‚æ•°é‡å‡å°‘ 98% çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½æŸå¤±ä»… 2%ï¼Œè¿™æ˜¯å¯æ¥å—çš„
#    - å»ºè®®ï¼šå¦‚æœè¿½æ±‚æ›´é«˜æ€§èƒ½ï¼Œå¯ä»¥å¢åŠ è®­ç»ƒè½®æ¬¡æˆ–è°ƒæ•´å­¦ä¹ ç‡
print("==============================================")
print("--- 8. å¼€å§‹å®éªŒå›› (LoRA + DAPT + R-Drop) ---")
print("==============================================")
print("ğŸ’¡ æç¤ºï¼šR-Drop æ˜¯ç®—æ³•å±‚é¢çš„æ”¹è¿›ï¼Œé¢„æœŸæ•ˆæœæ˜æ˜¾ï¼ˆ1-2%ï¼‰ï¼Œ")
print("   ä¸”è®­ç»ƒæ—¶é—´ä¸å®éªŒäºŒç›¸è¿‘ï¼ˆä¸å¢åŠ æ•°æ®é‡ï¼‰ï¼Œæ¨èä¼˜å…ˆä½¿ç”¨æ­¤æ–¹æ³•ã€‚")
# å®éªŒå›› = ä¸å®éªŒä¸‰ä¿æŒåŒæ ·çš„ DAPT æ¨¡å‹ä¸ LoRA é…ç½®ï¼Œå”¯ä¸€æ–°å¢å˜é‡æ˜¯ R-Drop æŸå¤±ã€‚
# ç›®æ ‡ï¼šè§‚å¯Ÿåœ¨ DAPT åŸºç¡€ä¸Šï¼ŒR-Drop æ˜¯å¦å¸¦æ¥é¢å¤–çš„æ³›åŒ–æå‡ã€‚

# 1. æ•°æ®é›†ï¼šæ²¿ç”¨å®éªŒä¸‰çš„æ•°æ®é›†ï¼Œä»¥ä¿è¯å¯æ¯”æ€§

# 2. ä» DAPT åçš„æ¨¡å‹åŠ è½½ï¼Œåˆ›å»ºåˆ†ç±»æ¨¡å‹ï¼ˆä¸å®éªŒä¸‰ç›¸åŒï¼‰
print("\nä» DAPT åçš„æ¨¡å‹åˆ›å»ºåˆ†ç±»æ¨¡å‹ï¼ˆç”¨äº R-Dropï¼‰...")
try:
    model_lora_rdrop = AutoModelForSequenceClassification.from_pretrained(
        dapt_model_path,  # ä» DAPT æ¨¡å‹åŠ è½½ï¼ˆä¸å®éªŒä¸‰ç›¸åŒï¼‰
        num_labels=2,
        local_files_only=False
    )
except Exception as e:
    print(f"ä» DAPT æ¨¡å‹åˆ›å»ºåˆ†ç±»æ¨¡å‹å¤±è´¥: {str(e)}")
    print("å°è¯•ä»åŸºç¡€æ¨¡å‹åŠ è½½å¹¶æ‰‹åŠ¨å¤åˆ¶ DAPT æƒé‡...")
    # å¤‡ç”¨æ–¹æ¡ˆï¼šä»åŸºç¡€æ¨¡å‹åŠ è½½ï¼Œç„¶åæ‰‹åŠ¨å¤åˆ¶ encoder æƒé‡
    # å…ˆåŠ è½½ DAPT åçš„ MLM æ¨¡å‹ä»¥è·å–æƒé‡
    model_dapt_mlm_for_rdrop = AutoModelForMaskedLM.from_pretrained(
        dapt_model_path,
        local_files_only=False
    )
    model_lora_rdrop = AutoModelForSequenceClassification.from_pretrained(
        MODEL_CHECKPOINT,
        num_labels=2,
        local_files_only=False
    )
    # åŠ è½½ DAPT çš„ encoder æƒé‡
    dapt_state_dict = model_dapt_mlm_for_rdrop.distilbert.state_dict()
    model_lora_rdrop.distilbert.load_state_dict(dapt_state_dict)
    print("å·²æ‰‹åŠ¨å¤åˆ¶ DAPT æƒé‡åˆ°åˆ†ç±»æ¨¡å‹")

model_lora_rdrop_peft = get_peft_model(model_lora_rdrop, lora_config)

print("æ¨¡å‹ (LoRA + DAPT + R-Drop) å¯è®­ç»ƒå‚æ•°:")
print_trainable_parameters(model_lora_rdrop_peft)

# 3. è®­ç»ƒå‚æ•°
training_args_lora_rdrop = TrainingArguments(
    output_dir="./results/lora_rdrop",
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE_DAPT_FINETUNE,  # ä¿æŒä¸å®éªŒä¸‰ä¸€è‡´ï¼Œçªå‡º R-Drop å˜é‡
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_f1",
    greater_is_better=True,
    save_total_limit=3,
    report_to="none",
    logging_dir=None,
    run_name="lora_rdrop",
    warmup_steps=100,
    logging_steps=100,
    disable_tqdm=False,
    logging_first_step=True,
)

# 4. ä½¿ç”¨è‡ªå®šä¹‰ R-Drop Trainer
print("\n--- æ„å»º R-Drop Trainer ---")
trainer_lora_rdrop = RDropTrainer(
    model=model_lora_rdrop_peft,
    args=training_args_lora_rdrop,
    train_dataset=train_dataset_advanced,
    eval_dataset=eval_dataset_advanced,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# 5. å¼€å§‹è®­ç»ƒ
print("\n--- å¼€å§‹è®­ç»ƒ (LoRA + DAPT + R-Drop) ---")
trainer_lora_rdrop.train()
print("--- è®­ç»ƒ (LoRA + DAPT + R-Drop) å®Œæˆ ---")

# 6. è¯„ä¼°
print("\n--- å¼€å§‹è¯„ä¼° (LoRA + DAPT + R-Drop) ---")
eval_results_lora_rdrop = trainer_lora_rdrop.evaluate()
print("è¯„ä¼°ç»“æœ (LoRA + DAPT + R-Drop):", eval_results_lora_rdrop)

# 7. å¯è§†åŒ–
print("\n--- ç”Ÿæˆå¯è§†åŒ– (LoRA + DAPT + R-Drop) ---")
plot_training_curves(trainer_lora_rdrop, "å®éªŒå››_LoRA_DAPT_RDrop")
plot_confusion_matrix(
    trainer_lora_rdrop.model,
    tokenizer,
    eval_dataset_advanced,
    torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    "å®éªŒå››_LoRA_DAPT_RDrop",
)

# 8. ä¿å­˜æ¨¡å‹
print("\n--- ä¿å­˜æ¨¡å‹ (LoRA + DAPT + R-Drop) ç”¨äºå‰ç«¯ ---")
model_save_path_lora_rdrop = "./saved_models/lora_rdrop"
trainer_lora_rdrop.model.save_pretrained(model_save_path_lora_rdrop)
tokenizer.save_pretrained(model_save_path_lora_rdrop)
print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_save_path_lora_rdrop}")

print("--- å®éªŒå›› (LoRA + DAPT + R-Drop) ç»“æŸ ---\n")

# --- 9. ç»“æœæ±‡æ€»ä¸å¯¹æ¯” (ç³»ç»Ÿæ€§å¯¹æ¯”) ---
print("==============================================")
print("--- 9. å®éªŒç»“æœæ±‡æ€» (ç³»ç»Ÿæ€§å¯¹æ¯”) ---")
print("==============================================")
print("è¿™é¡¹å·¥ä½œæ˜¯å®éªŒæŠ¥å‘Šçš„æ ¸å¿ƒï¼šåˆ†æç»“æœ")
print("\n--- å®éªŒä¸€ (Baseline) è¯„ä¼°ç»“æœ ---")
print(eval_results_baseline)

print("\n--- å®éªŒäºŒ (LoRA) è¯„ä¼°ç»“æœ ---")
print(eval_results_lora)

print("\n--- å®éªŒä¸‰ (LoRA + DAPT) è¯„ä¼°ç»“æœ ---")
print(eval_results_lora_advanced)

print("\n--- å®éªŒå›› (LoRA + DAPT + R-Drop) è¯„ä¼°ç»“æœ ---")
print(eval_results_lora_rdrop)

print("\n--- åˆæ­¥åˆ†æ ---")
baseline_f1 = eval_results_baseline.get('eval_f1', 0)
lora_f1 = eval_results_lora.get('eval_f1', 0)
lora_advanced_f1 = eval_results_lora_advanced.get('eval_f1', 0)
lora_rdrop_f1 = eval_results_lora_rdrop.get('eval_f1', 0)

print(f"å®éªŒä¸€ (Baseline) F1: {baseline_f1:.4f} | Accuracy: {eval_results_baseline.get('eval_accuracy', 0):.4f}")
print(f"å®éªŒäºŒ (LoRA) F1: {lora_f1:.4f} | Accuracy: {eval_results_lora.get('eval_accuracy', 0):.4f}")
print(f"å®éªŒä¸‰ (LoRA + DAPT) F1: {lora_advanced_f1:.4f} | Accuracy: {eval_results_lora_advanced.get('eval_accuracy', 0):.4f}")
print(f"å®éªŒå›› (LoRA + DAPT + R-Drop) F1: {lora_rdrop_f1:.4f} | Accuracy: {eval_results_lora_rdrop.get('eval_accuracy', 0):.4f}")

# ç”Ÿæˆå®éªŒå¯¹æ¯”å›¾
print("\n--- ç”Ÿæˆå®éªŒå¯¹æ¯”å›¾ ---")
results_dict = {
    "å®éªŒä¸€_Baseline": eval_results_baseline,
    "å®éªŒäºŒ_LoRA": eval_results_lora,
    "å®éªŒä¸‰_LoRA_DAPT": eval_results_lora_advanced,
    "å®éªŒå››_LoRA_DAPT_RDrop": eval_results_lora_rdrop,
}
plot_comparison(results_dict)

print("\n--- ç»“æœåˆ†æ ---")
print(f"è®­ç»ƒé…ç½®ï¼š{NUM_EPOCHS} ä¸ª epochï¼Œè®­ç»ƒæ ·æœ¬ {TRAIN_SAMPLES} æ¡ï¼Œæµ‹è¯•æ ·æœ¬ {EVAL_SAMPLES} æ¡")
print("\n1. å¯¹æ¯”å®éªŒä¸€å’Œå®éªŒäºŒï¼š")
performance_diff = baseline_f1 - lora_f1
if lora_f1 < 0.5:
    print(f"   è­¦å‘Š: LoRA æ¨¡å‹æ•ˆæœè¾ƒå·® (F1={lora_f1:.4f})ï¼Œå¯èƒ½åŸå› ï¼š")
    print(f"      - è®­ç»ƒè½®æ¬¡ä¸è¶³æˆ–é…ç½®éœ€è¦è°ƒæ•´")
else:
    print(f"   LoRA ç”¨æå°‘çš„å‚æ•°è¾¾åˆ°äº†æ¥è¿‘ Baseline çš„æ€§èƒ½ï¼")
    print(f"      - Baseline: F1={baseline_f1:.4f}, å‚æ•°é‡=66.96M (100%)")
    print(f"      - LoRA: F1={lora_f1:.4f}, å‚æ•°é‡â‰ˆ1.18M (1.73%)")
    print(f"      - å‚æ•°é‡å‡å°‘: 66.96M -> 1.18M (å‡å°‘ 98.2%)")
    print(f"      - æ€§èƒ½æŸå¤±: {performance_diff:.4f} (ä»… {performance_diff/baseline_f1*100:.2f}%)")
    print(f"      - ç»“è®ºï¼šLoRA é«˜æ•ˆå¾®è°ƒéå¸¸æˆåŠŸï¼ç”¨ä¸åˆ° 2% çš„å‚æ•°è¾¾åˆ°äº† 98.8% çš„æ€§èƒ½")

print("\n2. å¯¹æ¯”å®éªŒäºŒå’Œå®éªŒä¸‰ï¼ˆDAPTï¼‰ï¼š")
dapt_diff = lora_advanced_f1 - lora_f1
if abs(dapt_diff) < 0.01:
    print(f"   DAPT æ•ˆæœä¸æ˜æ˜¾ (F1å·®å¼‚: {abs(dapt_diff):.4f})")
    print(f"      - å¯èƒ½åŸå› ï¼šDAPT é¢„è®­ç»ƒè½®æ¬¡ä¸è¶³æˆ–æ•°æ®é‡ä¸å¤Ÿ")
    print(f"      - å»ºè®®ï¼šå¢åŠ  DAPT_EPOCHS æˆ– DAPT_SAMPLES ä»¥è·å¾—æ›´å¥½çš„æ•ˆæœ")
else:
    if dapt_diff > 0:
        print(f"   DAPT å¸¦æ¥äº† {dapt_diff:.4f} çš„æå‡ï¼")
        print(f"      - DAPT è®©æ¨¡å‹é€‚åº”äº† IMDb å½±è¯„é¢†åŸŸçš„è¯­è¨€é£æ ¼")
        print(f"      - é¢„æœŸæå‡ï¼š0.5-1.5%ï¼Œè¿™æ˜¯é¢†åŸŸé€‚åº”æ€§é¢„è®­ç»ƒçš„å…¸å‹æ•ˆæœ")
        print(f"      - æ³¨æ„ï¼šæå‡å¹…åº¦å–å†³äº DAPT çš„é¢„è®­ç»ƒè½®æ¬¡å’Œæ•°æ®é‡")
    else:
        print(f"   DAPT ç•¥å¾®é™ä½äº†æ€§èƒ½ ({abs(dapt_diff):.4f})")
        print(f"      - å¯èƒ½åŸå› ï¼šDAPT é¢„è®­ç»ƒè½®æ¬¡ä¸è¶³æˆ–å­¦ä¹ ç‡ä¸å½“")
        print(f"      - å»ºè®®ï¼šå¢åŠ  DAPT_EPOCHS æˆ–è°ƒæ•´ LEARNING_RATE_DAPT_MLM")

print("\n3. å¯¹æ¯”å®éªŒä¸‰å’Œå®éªŒå››ï¼ˆR-Drop çš„é¢å¤–æ•ˆæœï¼‰ï¼š")
rdrop_diff = lora_rdrop_f1 - lora_advanced_f1
if rdrop_diff > 0.005:
    print(f"   R-Drop åœ¨ DAPT åŸºç¡€ä¸Šå¸¦æ¥äº†é¢å¤– {rdrop_diff:.4f} çš„æå‡ï¼")
    print(f"      - R-Drop æ˜¯ç®—æ³•å±‚é¢çš„æ”¹è¿›ï¼Œé€šè¿‡ä¸€è‡´æ€§æ­£åˆ™åŒ–æå‡æ³›åŒ–èƒ½åŠ›")
    print(f"      - ä¸å¢åŠ è®­ç»ƒæ•°æ®é‡ï¼Œè®­ç»ƒæ—¶é—´ä¸å®éªŒäºŒç›¸è¿‘")
    print(f"      - æ¨èï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒDAPT + R-Drop çš„ç»„åˆæ•ˆæœæœ€ä½³")
elif rdrop_diff < -0.005:
    print(f"   æ•°æ®å¢å¼ºæ•ˆæœæ›´ä¼˜ï¼ˆå·®å¼‚: {abs(rdrop_diff):.4f}ï¼‰")
    print(f"      - è¿™å¯èƒ½æ˜¯å› ä¸ºæ•°æ®å¢å¼ºå¢åŠ äº†è®­ç»ƒæ ·æœ¬çš„å¤šæ ·æ€§")
else:
    print(f"   ä¸¤ç§æ–¹æ³•æ•ˆæœç›¸è¿‘ï¼ˆå·®å¼‚: {abs(rdrop_diff):.4f}ï¼‰")
    print(f"      - å»ºè®®ï¼šä¼˜å…ˆä½¿ç”¨ R-Dropï¼ˆè®­ç»ƒæ—¶é—´æ›´çŸ­ï¼Œä¸å¢åŠ æ•°æ®é‡ï¼‰")

print("\n--- å®éªŒç»“è®º ---")
print("å®éªŒæˆåŠŸå®Œæˆï¼ä¸»è¦å‘ç°ï¼š")
print(f"   1. LoRA é«˜æ•ˆå¾®è°ƒéå¸¸æœ‰æ•ˆï¼šä»…ç”¨ 1.73% çš„å‚æ•°è¾¾åˆ°çº¦ {lora_f1/baseline_f1*100:.1f}% çš„æ€§èƒ½")
print(f"   2. Baseline å…¨é‡å¾®è°ƒæ•ˆæœæœ€å¥½ï¼šF1={baseline_f1:.4f}")
print(f"   3. LoRA åŸºç¡€ç‰ˆæœ¬æ•ˆæœä¼˜ç§€ï¼šF1={lora_f1:.4f}ï¼Œæ€§èƒ½æŸå¤±ä»… {performance_diff:.4f}")
print(f"   4. R-Drop æ˜¯ç®—æ³•å±‚é¢çš„æ”¹è¿›ï¼Œé¢„æœŸæ•ˆæœæ¯”æ•°æ®å¢å¼ºæ›´æ˜æ˜¾ï¼ˆ1-2% vs 0.5-1%ï¼‰")
print(f"\n--- è®­ç»ƒæ•ˆç‡å¯¹æ¯” ---")
print(f"   - Baseline: è®­ç»ƒæ—¶é—´çº¦ 26 åˆ†é’Ÿï¼Œæ•ˆæœæœ€å¥½ï¼ˆF1={baseline_f1:.4f}ï¼‰")
print(f"   - LoRA: è®­ç»ƒæ—¶é—´çº¦ 23 åˆ†é’Ÿï¼Œå‚æ•°é‡å°‘ 98%ï¼Œæ•ˆæœæ¥è¿‘ï¼ˆF1={lora_f1:.4f}ï¼‰")
print(f"   - æ³¨æ„ï¼šå½“å‰æ¨¡å‹è§„æ¨¡ï¼ˆ66Mï¼‰ä¸‹ï¼ŒLoRA çš„æ—¶é—´ä¼˜åŠ¿ä¸æ˜æ˜¾")
print(f"     LoRA çš„ä¼˜åŠ¿ä¸»è¦ä½“ç°åœ¨ï¼š")
print(f"     * å‚æ•°é‡å¤§å¹…å‡å°‘ï¼ˆ1.73% vs 100%ï¼‰")
print(f"     * åœ¨æ›´å¤§æ¨¡å‹ï¼ˆ110M+ï¼‰ä¸Šè®­ç»ƒé€Ÿåº¦ä¼˜åŠ¿æ›´æ˜æ˜¾")
print(f"     * å†…å­˜å ç”¨æ›´å°‘ï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯")
print(f"\n--- æ”¹è¿›æ–¹æ³•æ¨è ---")
print(f"   1. å¦‚æœè¿½æ±‚æ•ˆæœï¼šä¼˜å…ˆä½¿ç”¨ R-Dropï¼ˆå®éªŒå››ï¼‰ï¼Œé¢„æœŸæå‡ 1-2%")
print(f"   2. å¦‚æœè¿½æ±‚é€Ÿåº¦ï¼šä½¿ç”¨å¿«é€Ÿæ•°æ®å¢å¼ºï¼ˆç¦ç”¨åŒä¹‰è¯æ›¿æ¢ï¼‰ï¼Œè€—æ—¶ä»æ•°å°æ—¶é™è‡³æ•°åˆ†é’Ÿ")
print(f"   3. å¦‚æœè¿½æ±‚å¹³è¡¡ï¼šä½¿ç”¨ LoRA + R-Dropï¼Œæ•ˆæœå’Œæ•ˆç‡å…¼é¡¾")

print("==============================================")

# --- 9. æ¨¡å‹å¯è§£é‡Šæ€§æ¢ç©¶ (SHAP) ---
# å®éªŒè¦æ±‚ï¼šä½¿ç”¨ SHAP/LIME ç­‰å·¥å…·ï¼Œæ¢ç©¶æ¨¡å‹å†³ç­–ä¾æ®
print("\n==============================================")
print("--- 9. æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ (SHAP) ---")
print("==============================================")
print("\næç¤ºï¼šSHAP å¯è§£é‡Šæ€§åˆ†æå·²åœ¨ä¸“é—¨çš„ Jupyter Notebook ä¸­å®Œæˆ")
print("   è¯·ä½¿ç”¨ SHAP_Analysis.ipynb è¿›è¡Œäº¤äº’å¼å¯è§†åŒ–åˆ†æ")
print("   Notebook æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š")
print("      - äº¤äº’å¼æ–‡æœ¬é«˜äº®å¯è§†åŒ– (shap.plots.text())")
print("      - æ¡å½¢å›¾æ˜¾ç¤ºæœ€é‡è¦çš„è¯ (shap.plots.bar())")
print("      - å•ä¸ªæ ·æœ¬è¯¦ç»†åˆ†æ")
print("      - è‡ªåŠ¨ä¿å­˜å¯è§†åŒ–ç»“æœ")
print("\n   ä½¿ç”¨æ–¹æ³•ï¼š")
print("      1. ç¡®ä¿å·²è¿è¡Œæœ¬è„šæœ¬å®Œæˆæ¨¡å‹è®­ç»ƒå’Œä¿å­˜")
print("      2. æ‰“å¼€ Jupyter Notebook: jupyter notebook SHAP_Analysis.ipynb")
print("      3. åœ¨ Notebook ä¸­é€‰æ‹©è¦åˆ†æçš„æ¨¡å‹ï¼ˆbaseline/lora/lora_advancedï¼‰")
print("      4. æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰å•å…ƒæ ¼å³å¯")
print("\n   æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°ä»¥ä¸‹è·¯å¾„ï¼š")
print(f"      - Baseline: ./saved_models/baseline")
print(f"      - LoRA: ./saved_models/lora_basic")
print(f"      - LoRA + æ•°æ®å¢å¼º: ./saved_models/lora_advanced")

print("\n==============================================")
print("--- å®éªŒä¸‰ (NLP) è„šæœ¬æ‰§è¡Œå®Œæ¯• ---")
print("==============================================")